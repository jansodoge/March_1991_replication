# socialization ======
for(n_ in 1:n) {
for(m_ in 1:m) {
if(runif(1) < p1) {
# individuals don't learn if code is 0 [page 74, line 27:28]
if(org_code[m_] != 0)  {
beliefs[n_, m_] <- org_code[m_]
}
}
}
}
# end: socialization
# code learning ======
# * choosing the chosen ones (superior knowledge) ######
# find out the quality of knowledge for the code and individuals
knowl_code <- sum(org_code == external_reality)
knowl_wkrs <- numeric(n)  # vector with num of correct dimensions
for(person in 1:n) {
knowl_wkrs[person] <- sum(beliefs[person,] == external_reality)
}
chosen_ones <- numeric(n)  # individuals smarter than the org code
# in the beginning pretty much all workers are better than the code
# which has zero knowledge
for(person in 1:n) {
if(knowl_code < knowl_wkrs[person]) {
chosen_ones[person] <- 1
}
}
# *superior matrix  ######
if(sum(chosen_ones) > 0) {
knowl_matrix <- matrix(0, nrow=sum(chosen_ones), ncol=m)
p <- 1  # counter for the superior people
for(person in 1:n) {
if(chosen_ones[person] == 1) {
knowl_matrix[p,] <- beliefs[person,]
p <- p + 1
}
}
# *learning #####
# Setting up the superior group and its knowledge for the code to learn from it
# As per the paper, org code learns from the majority vote of superior
# group.
for(dimension in 1:m) {
result <- getvote(knowl_matrix[,dimension])
vote <- result[1]
k <- result[2]
if(runif(1) > ((1- p2)^k)) {
org_code[dimension] <- vote
}
}
}
#print(sum((org_code == external_reality)/m))
time_series <- append(time_series, sum((org_code == external_reality)/m))
# end: code learning from the individuals
}
# recording results  ======
# For now a crude but working version.
knowl_code <- sum(org_code == external_reality)
OUTPUT[c_p1, c_p2] <- OUTPUT[c_p1, c_p2] + (knowl_code/m)
}
}
}
OUTPUT <- OUTPUT/iterations  # taking the average
# SAVING RESULTS TO CSV ----
rownames(OUTPUT) <- P1_list
colnames(OUTPUT) <- P2_list
OUTPUT <- as.data.frame(OUTPUT)
OUTPUT$p1 <- row.names(OUTPUT)
OUTPUT$turnover_rate <- p3
turnover_fig_data <- dplyr::bind_rows(turnover_fig_data, OUTPUT)
}
data_averaged <- cbind(data_averaged, time_series)
}
averaged_data_vec <- rowMeans(data_averaged)
plot(averaged_data_vec, type="l")
# March 1991
# This is a recreation of the March 1991 model written in R.
# @author: Maciej Workiewicz (ESSEC)
# date: August 10, 2017
# FUNCTIONS ----------
getvote <- function(v) {
uniqv <- sort(unique(v[v!=0]))  # only sort the non 0 elements
freq <- tabulate(match(v, uniqv))  # find number of occurences
indx_max <- which(freq == max(freq))
k <- max(freq) - min(freq)
if(length(uniqv) == 1) {
# Here we need to eliminate an situation where with only one vote
# the max == min and thus k would be 0 (no code learning)
k <- max(freq)
}
output_ = c(uniqv[indx_max], k)
return(output_)
}
data_averaged <- c()
for(run in seq(1, 80)){
# MODEL VARIABLES  ------------------------
iterations = 1  # number of iteration, originally set to 80 in the paper
time = 100  # iterations until equilibrium, a lazy solution to the steady state problem
m <- 30  # number of dimensions
n <- 50  # number of people
p3_list <- c(0.0) # turnover
p4 <- 0.02  # environmental turbulence
# lists forming the parmeter space for the learning rates
P1_list <- c(0.5)
P2_list <- c(0.5)
turnover_fig_data <- data.frame()
for(turnover_rate in p3_list){
p3 <- turnover_rate
time_series <- c()
# PREPARING THE OUTPUT MATRIX ------
OUTPUT = matrix(0, nrow=length(P1_list), ncol=length(P2_list))
c_p1 <- 0  # counter for recording rows in the OUTPUT file
scenario <- 0  # counter to report progress
# SIMULATION  -----------------
for(p1 in P1_list) {
c_p1 <- c_p1 + 1
c_p2 <- 0  # counter for recording columns in the OUTPUT file
for(p2 in P2_list) {
c_p2 <- c_p2 + 1
scenario <- scenario + 1
for(i in 1:iterations) {
cat("\r","Scenario: ", toString(scenario), " out of ", toString(length(P1_list)*length(P2_list)), ", iteration: ", toString(i))
# Generating starting objects
external_reality <- 2*(floor(runif(m, min=0, max=2))) - 1
beliefs <- matrix(floor(runif(n*m, min=-1, max=2)), nrow=n, ncol=m)
org_code <- numeric(m)
for(t in 1:time) {
# turnover =====
for(x3 in 1:n) {
if(runif(1) < p3){
beliefs[x3,] <- floor(runif(m, min=-1, max=2))
}
}
# environmental turbulence  =====
for(x4 in 1:m) {
if(runif(1) < p4) {
external_reality[x4] <- external_reality[x4]*(-1)
}
}
# socialization ======
for(n_ in 1:n) {
for(m_ in 1:m) {
if(runif(1) < p1) {
# individuals don't learn if code is 0 [page 74, line 27:28]
if(org_code[m_] != 0)  {
beliefs[n_, m_] <- org_code[m_]
}
}
}
}
# end: socialization
# code learning ======
# * choosing the chosen ones (superior knowledge) ######
# find out the quality of knowledge for the code and individuals
knowl_code <- sum(org_code == external_reality)
knowl_wkrs <- numeric(n)  # vector with num of correct dimensions
for(person in 1:n) {
knowl_wkrs[person] <- sum(beliefs[person,] == external_reality)
}
chosen_ones <- numeric(n)  # individuals smarter than the org code
# in the beginning pretty much all workers are better than the code
# which has zero knowledge
for(person in 1:n) {
if(knowl_code < knowl_wkrs[person]) {
chosen_ones[person] <- 1
}
}
# *superior matrix  ######
if(sum(chosen_ones) > 0) {
knowl_matrix <- matrix(0, nrow=sum(chosen_ones), ncol=m)
p <- 1  # counter for the superior people
for(person in 1:n) {
if(chosen_ones[person] == 1) {
knowl_matrix[p,] <- beliefs[person,]
p <- p + 1
}
}
# *learning #####
# Setting up the superior group and its knowledge for the code to learn from it
# As per the paper, org code learns from the majority vote of superior
# group.
for(dimension in 1:m) {
result <- getvote(knowl_matrix[,dimension])
vote <- result[1]
k <- result[2]
if(runif(1) > ((1- p2)^k)) {
org_code[dimension] <- vote
}
}
}
#print(sum((org_code == external_reality)/m))
time_series <- append(time_series, sum((org_code == external_reality)/m))
# end: code learning from the individuals
}
# recording results  ======
# For now a crude but working version.
knowl_code <- sum(org_code == external_reality)
OUTPUT[c_p1, c_p2] <- OUTPUT[c_p1, c_p2] + (knowl_code/m)
}
}
}
OUTPUT <- OUTPUT/iterations  # taking the average
# SAVING RESULTS TO CSV ----
rownames(OUTPUT) <- P1_list
colnames(OUTPUT) <- P2_list
OUTPUT <- as.data.frame(OUTPUT)
OUTPUT$p1 <- row.names(OUTPUT)
OUTPUT$turnover_rate <- p3
turnover_fig_data <- dplyr::bind_rows(turnover_fig_data, OUTPUT)
}
data_averaged <- cbind(data_averaged, time_series)
}
averaged_data_vec <- rowMeans(data_averaged)
plot(averaged_data_vec, type="l")
# March 1991
# This is a recreation of the March 1991 model written in R.
# @author: Maciej Workiewicz (ESSEC)
# date: August 10, 2017
# FUNCTIONS ----------
getvote <- function(v) {
uniqv <- sort(unique(v[v!=0]))  # only sort the non 0 elements
freq <- tabulate(match(v, uniqv))  # find number of occurences
indx_max <- which(freq == max(freq))
k <- max(freq) - min(freq)
if(length(uniqv) == 1) {
# Here we need to eliminate an situation where with only one vote
# the max == min and thus k would be 0 (no code learning)
k <- max(freq)
}
output_ = c(uniqv[indx_max], k)
return(output_)
}
data_averaged <- c()
for(run in seq(1, 80)){
# MODEL VARIABLES  ------------------------
iterations = 1  # number of iteration, originally set to 80 in the paper
time = 100  # iterations until equilibrium, a lazy solution to the steady state problem
m <- 30  # number of dimensions
n <- 50  # number of people
p3_list <- c(0.0) # turnover
p4 <- 0.02  # environmental turbulence
# lists forming the parmeter space for the learning rates
P1_list <- c(0.5)
P2_list <- c(0.5)
turnover_fig_data <- data.frame()
for(turnover_rate in p3_list){
p3 <- turnover_rate
time_series <- c()
# PREPARING THE OUTPUT MATRIX ------
OUTPUT = matrix(0, nrow=length(P1_list), ncol=length(P2_list))
c_p1 <- 0  # counter for recording rows in the OUTPUT file
scenario <- 0  # counter to report progress
# SIMULATION  -----------------
for(p1 in P1_list) {
c_p1 <- c_p1 + 1
c_p2 <- 0  # counter for recording columns in the OUTPUT file
for(p2 in P2_list) {
c_p2 <- c_p2 + 1
scenario <- scenario + 1
for(i in 1:iterations) {
cat("\r","Scenario: ", toString(scenario), " out of ", toString(length(P1_list)*length(P2_list)), ", iteration: ", toString(i))
# Generating starting objects
external_reality <- 2*(floor(runif(m, min=0, max=2))) - 1
beliefs <- matrix(floor(runif(n*m, min=-1, max=2)), nrow=n, ncol=m)
org_code <- numeric(m)
for(t in 1:time) {
# turnover =====
for(x3 in 1:n) {
if(runif(1) < p3){
beliefs[x3,] <- floor(runif(m, min=-1, max=2))
}
}
# environmental turbulence  =====
for(x4 in 1:m) {
if(runif(1) < p4) {
external_reality[x4] <- external_reality[x4]*(-1)
}
}
# socialization ======
for(n_ in 1:n) {
for(m_ in 1:m) {
if(runif(1) < p1) {
# individuals don't learn if code is 0 [page 74, line 27:28]
if(org_code[m_] != 0)  {
beliefs[n_, m_] <- org_code[m_]
}
}
}
}
# end: socialization
# code learning ======
# * choosing the chosen ones (superior knowledge) ######
# find out the quality of knowledge for the code and individuals
knowl_code <- sum(org_code == external_reality)
knowl_wkrs <- numeric(n)  # vector with num of correct dimensions
for(person in 1:n) {
knowl_wkrs[person] <- sum(beliefs[person,] == external_reality)
}
chosen_ones <- numeric(n)  # individuals smarter than the org code
# in the beginning pretty much all workers are better than the code
# which has zero knowledge
for(person in 1:n) {
if(knowl_code < knowl_wkrs[person]) {
chosen_ones[person] <- 1
}
}
# *superior matrix  ######
if(sum(chosen_ones) > 0) {
knowl_matrix <- matrix(0, nrow=sum(chosen_ones), ncol=m)
p <- 1  # counter for the superior people
for(person in 1:n) {
if(chosen_ones[person] == 1) {
knowl_matrix[p,] <- beliefs[person,]
p <- p + 1
}
}
# *learning #####
# Setting up the superior group and its knowledge for the code to learn from it
# As per the paper, org code learns from the majority vote of superior
# group.
for(dimension in 1:m) {
result <- getvote(knowl_matrix[,dimension])
vote <- result[1]
k <- result[2]
if(runif(1) > ((1- p2)^k)) {
org_code[dimension] <- vote
}
}
}
#print(sum((org_code == external_reality)/m))
time_series <- append(time_series, sum((org_code == external_reality)/m))
# end: code learning from the individuals
}
# recording results  ======
# For now a crude but working version.
knowl_code <- sum(org_code == external_reality)
OUTPUT[c_p1, c_p2] <- OUTPUT[c_p1, c_p2] + (knowl_code/m)
}
}
}
OUTPUT <- OUTPUT/iterations  # taking the average
# SAVING RESULTS TO CSV ----
rownames(OUTPUT) <- P1_list
colnames(OUTPUT) <- P2_list
OUTPUT <- as.data.frame(OUTPUT)
OUTPUT$p1 <- row.names(OUTPUT)
OUTPUT$turnover_rate <- p3
turnover_fig_data <- dplyr::bind_rows(turnover_fig_data, OUTPUT)
}
data_averaged <- cbind(data_averaged, time_series)
}
averaged_data_vec <- rowMeans(data_averaged)
plot(averaged_data_vec, type="l")
time_series_wo_turnover <- averaged_data_vec
simulated_data <- data.frame(time = seq(1,100),
wo_turnover = time_series_wo_turnover,
w_turnover = time_series_w_turnover)
simulated_data <- data.frame(time = seq(1,100),
wo_turnover = time_series_wo_turnover,
w_turnover = time_series_w_turnover) %>%
pivot_longer(time, names_to = "type", values_to = "average_code_knowledge")
str(simulated_data)
View(simulated_data)
####
simulated_data <- data.frame(time = seq(1,100),
wo_turnover = time_series_wo_turnover,
w_turnover = time_series_w_turnover) %>%
pivot_longer(!time, names_to = "type", values_to = "average_code_knowledge")
ggplot(data = simulated_data, aes(x = time, y = average_code_knowledge, fill = type))+
geom_line()
ggplot(data = simulated_data, aes(x = time, y = average_code_knowledge, fill = type, color = type))+
geom_line()
ggplot(data = simulated_data, aes(x = time, y = average_code_knowledge, fill = type, color = type))+
geom_line()+
xlab("Time Period")+
ylab("Average Code Knowledge")
# March 1991
# This is a recreation of the March 1991 model written in R.
# @author: Maciej Workiewicz (ESSEC)
# date: August 10, 2017
# FUNCTIONS ----------
getvote <- function(v) {
uniqv <- sort(unique(v[v!=0]))  # only sort the non 0 elements
freq <- tabulate(match(v, uniqv))  # find number of occurences
indx_max <- which(freq == max(freq))
k <- max(freq) - min(freq)
if(length(uniqv) == 1) {
# Here we need to eliminate an situation where with only one vote
# the max == min and thus k would be 0 (no code learning)
k <- max(freq)
}
output_ = c(uniqv[indx_max], k)
return(output_)
}
# MODEL VARIABLES  ------------------------
iterations = 80  # number of iteration, originally set to 80 in the paper
time = 20  # iterations until equilibrium, a lazy solution to the steady state problem
m <- 30  # number of dimensions
n <- 50  # number of people
p3_list <- c(seq(0,1,0.1)) # turnover
p4 <- 0  # environmental turbulence
# lists forming the parmeter space for the learning rates
P1_list <- c(0.1, 0.9)
P2_list <- c(0.5)
turnover_fig_data <- data.frame()
for(turnover_rate in p3_list){
p3 <- turnover_rate
# PREPARING THE OUTPUT MATRIX ------
OUTPUT = matrix(0, nrow=length(P1_list), ncol=length(P2_list))
c_p1 <- 0  # counter for recording rows in the OUTPUT file
scenario <- 0  # counter to report progress
# SIMULATION  -----------------
for(p1 in P1_list) {
c_p1 <- c_p1 + 1
c_p2 <- 0  # counter for recording columns in the OUTPUT file
for(p2 in P2_list) {
c_p2 <- c_p2 + 1
scenario <- scenario + 1
for(i in 1:iterations) {
cat("\r","Scenario: ", toString(scenario), " out of ", toString(length(P1_list)*length(P2_list)), ", iteration: ", toString(i))
# Generating starting objects
external_reality <- 2*(floor(runif(m, min=0, max=2))) - 1
beliefs <- matrix(floor(runif(n*m, min=-1, max=2)), nrow=n, ncol=m)
org_code <- numeric(m)
for(t in 1:time) {
# turnover =====
for(x3 in 1:n) {
if(runif(1) < p3){
beliefs[x3,] <- floor(runif(m, min=-1, max=2))
}
}
# environmental turbulence  =====
for(x4 in 1:m) {
if(runif(1) < p4) {
external_reality[x4] <- external_reality[x4]*(-1)
}
}
# socialization ======
for(n_ in 1:n) {
for(m_ in 1:m) {
if(runif(1) < p1) {
# individuals don't learn if code is 0 [page 74, line 27:28]
if(org_code[m_] != 0)  {
beliefs[n_, m_] <- org_code[m_]
}
}
}
}
# end: socialization
# code learning ======
# * choosing the chosen ones (superior knowledge) ######
# find out the quality of knowledge for the code and individuals
knowl_code <- sum(org_code == external_reality)
knowl_wkrs <- numeric(n)  # vector with num of correct dimensions
for(person in 1:n) {
knowl_wkrs[person] <- sum(beliefs[person,] == external_reality)
}
chosen_ones <- numeric(n)  # individuals smarter than the org code
# in the beginning pretty much all workers are better than the code
# which has zero knowledge
for(person in 1:n) {
if(knowl_code < knowl_wkrs[person]) {
chosen_ones[person] <- 1
}
}
# *superior matrix  ######
if(sum(chosen_ones) > 0) {
knowl_matrix <- matrix(0, nrow=sum(chosen_ones), ncol=m)
p <- 1  # counter for the superior people
for(person in 1:n) {
if(chosen_ones[person] == 1) {
knowl_matrix[p,] <- beliefs[person,]
p <- p + 1
}
}
# *learning #####
# Setting up the superior group and its knowledge for the code to learn from it
# As per the paper, org code learns from the majority vote of superior
# group.
for(dimension in 1:m) {
result <- getvote(knowl_matrix[,dimension])
vote <- result[1]
k <- result[2]
if(runif(1) > ((1- p2)^k)) {
org_code[dimension] <- vote
}
}
}
# end: code learning from the individuals
}
# recording results  ======
# For now a crude but working version.
knowl_code <- sum(org_code == external_reality)
OUTPUT[c_p1, c_p2] <- OUTPUT[c_p1, c_p2] + (knowl_code/m)
}
}
}
OUTPUT <- OUTPUT/iterations  # taking the average
# SAVING RESULTS TO CSV ----
rownames(OUTPUT) <- P1_list
colnames(OUTPUT) <- P2_list
OUTPUT <- as.data.frame(OUTPUT)
OUTPUT$p1 <- row.names(OUTPUT)
OUTPUT$turnover_rate <- p3
turnover_fig_data <- dplyr::bind_rows(turnover_fig_data, OUTPUT)
}
ggplot(data = turnover_fig_data, aes(x = turnover_rate, y = `0.5`, color = p1))+
geom_line()+
ylab("Average Period-20 Code Knowledge")+
xlab("Turnover Rate")+
ylim(c(0.5,0.9))
save(turnover_fig_data, file = "simulation_data_fig4.RData")
